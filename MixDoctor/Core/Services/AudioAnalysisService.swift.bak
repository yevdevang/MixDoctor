//
//  AudioAnalysisService.swift
//  MixDoctor
//
//  Main service for audio analysis orchestration
//

import Foundation
import Observation

@Observable
final class AudioAnalysisService {
    
    private let processor = AudioProcessor()
    private let featureExtractor = AudioFeatureExtractor()
    private let separationService = AudioSourceSeparationService()
    
    var isAnalyzing: Bool = false
    var analysisProgress: Double = 0
    
    // Configuration
    var enableStemAnalysis: Bool = true  // Can be disabled for faster analysis
    
    // MARK: - Main Analysis
    
    func analyzeAudio(_ audioFile: AudioFile) async throws -> AnalysisResult {
        isAnalyzing = true
        analysisProgress = 0
        
        defer {
            isAnalyzing = false
            analysisProgress = 0
        }
        
        // Verify file exists before attempting analysis
        let fileURL = audioFile.fileURL
        
        // Try both the original path and potential URL-decoded version
        var fileExists = FileManager.default.fileExists(atPath: fileURL.path)
        var actualURL = fileURL
        
        // If not found, try looking for files in the directory that match the decoded name
        if !fileExists {
            let fileName = fileURL.lastPathComponent
            let directory = fileURL.deletingLastPathComponent()
            
            if let contents = try? FileManager.default.contentsOfDirectory(atPath: directory.path) {
                // Look for a file that matches when URL-decoded
                if let matchingFile = contents.first(where: { $0 == fileName.removingPercentEncoding }) {
                    actualURL = directory.appendingPathComponent(matchingFile)
                    fileExists = FileManager.default.fileExists(atPath: actualURL.path)
                    if fileExists {
                        print("   ‚úÖ Found file with decoded name: \(matchingFile)")
                    }
                }
            }
        }
        
        print("üîç Pre-analysis file check:")
        print("   File URL: \(fileURL)")
        print("   File path: \(fileURL.path)")
        print("   Actual URL: \(actualURL)")
        print("   File exists: \(fileExists)")
        
        // If file doesn't exist, list directory contents for debugging
        if !fileExists {
            let directory = fileURL.deletingLastPathComponent()
            print("‚ùå File not found. Directory: \(directory.path)")
            if let contents = try? FileManager.default.contentsOfDirectory(atPath: directory.path) {
                print("   Files in directory: \(contents.count)")
                contents.prefix(10).forEach { print("   - \($0)") }
            }
        }
        
        guard fileExists else {
            throw NSError(domain: "AudioAnalysisService", code: 404, userInfo: [
                NSLocalizedDescriptionKey: "Audio file not found at path: \(fileURL.path). Please delete and re-import this file."
            ])
        }
        
        // Load and process audio using the actual URL that exists
        analysisProgress = 0.1
        let processedAudio = try processor.loadAudio(from: actualURL)
        
        // Extract features
        analysisProgress = 0.3
        let stereoFeatures = featureExtractor.extractStereoFeatures(
            left: processedAudio.leftChannel,
            right: processedAudio.rightChannel
        )
        
        analysisProgress = 0.5
        let frequencyFeatures = try featureExtractor.extractFrequencyFeatures(
            audio: processedAudio.leftChannel,
            sampleRate: processedAudio.sampleRate
        )
        
        analysisProgress = 0.7
        let loudnessFeatures = featureExtractor.extractLoudnessFeatures(
            left: processedAudio.leftChannel,
            right: processedAudio.rightChannel
        )
        
        // Extract mixing effects features
        analysisProgress = 0.75
        let mixingEffects = try featureExtractor.extractMixingEffects(
            left: processedAudio.leftChannel,
            right: processedAudio.rightChannel,
            sampleRate: processedAudio.sampleRate
        )
        
        print("   üéØ MIXING EFFECTS ANALYSIS:")
        print("      Compression: \(mixingEffects.hasCompression ? "YES ‚úì" : "NO ‚ùå") (amount: \(String(format: "%.1f", mixingEffects.compressionAmount * 100))%, crest: \(String(format: "%.1f", loudnessFeatures.crestFactor)))")
        print("      Reverb: \(mixingEffects.hasReverb ? "YES ‚úì" : "NO ‚ùå") (amount: \(String(format: "%.1f", mixingEffects.reverbAmount * 100))%)")
        print("      Stereo Processing: \(mixingEffects.hasStereoProcessing ? "YES ‚úì" : "NO ‚ùå") (enhancement: \(String(format: "%.1f", mixingEffects.stereoEnhancement * 100))%)")
        print("      EQ/Frequency Shaping: \(mixingEffects.hasEQ ? "YES ‚úì" : "NO ‚ùå") (balance: \(String(format: "%.1f", mixingEffects.frequencyBalance * 100))%)")
        
        // Show count of missing effects (for unmixed detection)
        let missingEffectsCount = [
            !mixingEffects.hasCompression,
            !mixingEffects.hasReverb,
            !mixingEffects.hasStereoProcessing,
            !mixingEffects.hasEQ
        ].filter { $0 }.count
        print("      ‚ö†Ô∏è MISSING EFFECTS: \(missingEffectsCount)/4 \(missingEffectsCount >= 3 ? "‚Üí UNMIXED!" : "")")
        
        // OPTIONAL: Perform stem-based analysis (Pro feature or user-enabled)
        var stemAnalysis: AudioFeatureExtractor.StemBasedMixAnalysis?
        var mixBalance: MixBalanceMetrics?
        
        let subscriptionService = await SubscriptionService.shared
        let isProUser = await subscriptionService.isProUser
        
        if enableStemAnalysis && isProUser {
            print("   üé∏ Performing stem-based analysis (Pro feature)...")
            analysisProgress = 0.8
            
            do {
                let separatedStems = try await separationService.separateAudio(from: actualURL)
                
                // Analyze mix balance from stems
                mixBalance = separationService.analyzeMixBalance(stems: separatedStems)
                
                // Detailed stem analysis
                stemAnalysis = try featureExtractor.analyzeMixFromStems(
                    vocals: separatedStems.vocals.map { ($0.left, $0.right) },
                    drums: separatedStems.drums.map { ($0.left, $0.right) },
                    bass: separatedStems.bass.map { ($0.left, $0.right) },
                    other: separatedStems.other.map { ($0.left, $0.right) },
                    sampleRate: processedAudio.sampleRate
                )
                
                print("   ‚úÖ Stem analysis complete")
            } catch {
                print("   ‚ö†Ô∏è Stem analysis failed: \(error.localizedDescription)")
                print("   ‚Üí Continuing with standard analysis")
            }
        } else {
            print("   ‚ÑπÔ∏è Stem analysis disabled (Pro feature or manually disabled)")
        }
        
        analysisProgress = 0.85
        
        // Calculate technical metrics
        let peakLevelDB = 20 * log10(loudnessFeatures.peakLevel + 0.0001)
        let rmsLevelDB = 20 * log10(loudnessFeatures.rmsLevel + 0.0001)
        
        // Extract frequency band energies (use correct band ranges)
        // Available bands: 20 (sub_bass), 60 (bass), 250 (low_mids), 500 (mids), 2000 (high_mids), 6000 (highs)
        let subBass: Float = frequencyFeatures.frequencyBands[20.0] ?? 0
        let bass: Float = frequencyFeatures.frequencyBands[60.0] ?? 0
        let lowMids: Float = frequencyFeatures.frequencyBands[250.0] ?? 0
        let mids: Float = frequencyFeatures.frequencyBands[500.0] ?? 0
        let highMids: Float = frequencyFeatures.frequencyBands[2000.0] ?? 0
        let highs: Float = frequencyFeatures.frequencyBands[6000.0] ?? 0
        
        // Combine for low/mid/high classification
        let lowEnergy: Float = (subBass + bass) / 2.0  // 20-250 Hz
        let midEnergy: Float = (lowMids + mids) / 2.0  // 250-2000 Hz
        let highEnergy: Float = (highMids + highs) / 2.0  // 2000-20000 Hz
        
        print("   üìä Extracted Features:")
        print("      Peak Level: \(peakLevelDB) dBFS")
        print("      RMS Level: \(rmsLevelDB) dBFS")
        print("      Dynamic Range: \(loudnessFeatures.dynamicRange) dB")
        print("      Stereo Width: \(stereoFeatures.stereoWidth)")
        print("      Phase Coherence: \(stereoFeatures.correlation)")
        print("      Frequency Bands:")
        print("        Sub Bass (20-60): \(subBass)")
        print("        Bass (60-250): \(bass)")
        print("        Low Mids (250-500): \(lowMids)")
        print("        Mids (500-2k): \(mids)")
        print("        High Mids (2k-6k): \(highMids)")
        print("        Highs (6k-20k): \(highs)")
        print("      Combined:")
        print("        Low: \(lowEnergy)")
        print("        Mid: \(midEnergy)")
        print("        High: \(highEnergy)")
        print("      Spectral Centroid: \(frequencyFeatures.spectralCentroid) Hz")
        
        // Analyze with OpenAI
        analysisProgress = 0.9
        
        // Use the already-fetched subscription status from stem analysis
        print("   ü§ñ Analyzing with OpenAI \(isProUser ? "GPT-4o (Pro)" : "GPT-4o-mini (Free)")...")
        print("   üì§ Sending to OpenAI:")
        print("      Peak Level: \(peakLevelDB) dBFS")
        print("      RMS Level: \(rmsLevelDB) dBFS")
        print("      Dynamic Range: \(loudnessFeatures.dynamicRange) dB")
        print("      Stereo Width: \(stereoFeatures.stereoWidth * 100)%")
        print("      Phase Coherence: \(stereoFeatures.correlation)")
        print("      Low Energy: \(lowEnergy)")
        print("      Mid Energy: \(midEnergy)")
        print("      High Energy: \(highEnergy)")
        print("      Spectral Centroid: \(frequencyFeatures.spectralCentroid) Hz")
        
        // Extract mixing effects detection (for unmixed track detection)
        let mixingEffects = try featureExtractor.extractMixingEffects(left: leftChannel, right: rightChannel, sampleRate: sampleRate)
        print("   ÔøΩ MIXING EFFECTS ANALYSIS:")
        print("      Compression: \(mixingEffects.hasCompression ? "YES ‚úì" : "NO ‚ùå") (amount: \(String(format: "%.1f", mixingEffects.compressionAmount * 100))%, crest: \(String(format: "%.1f", loudnessFeatures.crestFactor)))")
        print("      Reverb: \(mixingEffects.hasReverb ? "YES ‚úì" : "NO ‚ùå") (amount: \(String(format: "%.1f", mixingEffects.reverbAmount * 100))%)")
        print("      Stereo Processing: \(mixingEffects.hasStereoProcessing ? "YES ‚úì" : "NO ‚ùå") (enhancement: \(String(format: "%.1f", mixingEffects.stereoEnhancement * 100))%)")
        print("      EQ/Frequency Shaping: \(mixingEffects.hasEQ ? "YES ‚úì" : "NO ‚ùå") (balance: \(String(format: "%.1f", mixingEffects.frequencyBalance * 100))%)")
        
        // Show count of missing effects (for unmixed detection)
        let missingEffectsCount = [
            !mixingEffects.hasCompression,
            !mixingEffects.hasReverb,
            !mixingEffects.hasStereoProcessing,
            !mixingEffects.hasEQ
        ].filter { $0 }.count
        print("      ‚ö†Ô∏è MISSING EFFECTS: \(missingEffectsCount)/4 \(missingEffectsCount >= 3 ? "‚Üí UNMIXED!" : "")")
        
        // UNMIXED DETECTION CHECKS (log for debugging)
        print("   üö® UNMIXED DETECTION CHECKS:")
        print("      Stereo Width < 30%? \(stereoFeatures.stereoWidth * 100 < 30 ? "YES ‚ùå" : "NO ‚úì") (current: \(Int(stereoFeatures.stereoWidth * 100))%)")
        print("      Phase < 0.6? \(stereoFeatures.correlation < 0.6 ? "YES ‚ùå" : "NO ‚úì") (current: \(String(format: "%.2f", stereoFeatures.correlation)))")
        print("      Dynamic Range > 15 dB? \(loudnessFeatures.dynamicRange > 15 ? "YES ‚ùå" : "NO ‚úì") (current: \(String(format: "%.1f", loudnessFeatures.dynamicRange)) dB)")
        print("      RMS < -25 dBFS? \(rmsLevelDB < -25 ? "YES ‚ùå" : "NO ‚úì") (current: \(String(format: "%.1f", rmsLevelDB)) dBFS)")
        print("      Spectral Centroid < 800 Hz? \(frequencyFeatures.spectralCentroid < 800 ? "YES ‚ùå" : "NO ‚úì") (current: \(Int(frequencyFeatures.spectralCentroid)) Hz)")
        let maxFreq = max(lowEnergy, midEnergy, highEnergy)
        print("      ANY freq > 50%? \(maxFreq > 0.5 ? "YES ‚ùå" : "NO ‚úì") (max: \(String(format: "%.1f", maxFreq * 100))%)")
        print("      NO Compression? \(!mixingEffects.hasCompression ? "YES ‚ùå" : "NO ‚úì")")
        print("      NO Reverb? \(!mixingEffects.hasReverb ? "YES ‚ùå" : "NO ‚úì")")
        print("      NO Stereo Processing? \(!mixingEffects.hasStereoProcessing ? "YES ‚ùå" : "NO ‚úì")")
        print("      NO EQ? \(!mixingEffects.hasEQ ? "YES ‚ùå" : "NO ‚úì")")
        
        let unmixedIndicatorCount = [
            stereoFeatures.stereoWidth * 100 < 30,
            stereoFeatures.correlation < 0.6,
            loudnessFeatures.dynamicRange > 15,
            rmsLevelDB < -25,
            frequencyFeatures.spectralCentroid < 800,
            maxFreq > 0.5,
            !mixingEffects.hasCompression,    // No compression = unmixed
            !mixingEffects.hasReverb,         // No reverb = unmixed
            !mixingEffects.hasStereoProcessing, // No stereo processing = unmixed
            !mixingEffects.hasEQ              // No EQ = unmixed
        ].filter { $0 }.count
        
        print("      ‚ö†Ô∏è UNMIXED INDICATORS: \(unmixedIndicatorCount)/10 \(unmixedIndicatorCount > 0 ? "‚Üí SHOULD SCORE 35-50" : "‚Üí OK for higher score")")
        
        // Prepare stem metrics if available, OR create basic metrics with mixing effects detection
        var stemMetricsForAI: StemMetrics?
        if let balance = mixBalance, let analysis = stemAnalysis {
            // Full stem analysis available (Pro users)
            stemMetricsForAI = StemMetrics(
                vocalsLevel: balance.vocalsLevel,
                drumsLevel: balance.drumsLevel,
                bassLevel: balance.bassLevel,
                otherLevel: balance.otherLevel,
                mixDepth: analysis.depthScore,
                foregroundClarity: analysis.foregroundClarity,
                elementSeparation: analysis.elementSeparationQuality,
                frequencyMasking: analysis.frequencyMasking,
                mixDensity: analysis.mixDensity,
                vocalsStereoWidth: balance.vocalsStereoWidth,
                drumsStereoWidth: balance.drumsStereoWidth,
                bassStereoWidth: balance.bassStereoWidth,
                vocalsPlacement: analysis.vocalsPlacement,
                drumsPlacement: analysis.drumsPlacement,
                bassPlacement: analysis.bassPlacement,
                vocalsToInstrumentsRatio: analysis.vocalsToInstrumentsRatio,
                drumsToMixRatio: analysis.drumsToMixRatio,
                bassToMixRatio: analysis.bassToMixRatio,
                hasCompression: mixingEffects.hasCompression,
                compressionAmount: mixingEffects.compressionAmount,
                hasReverb: mixingEffects.hasReverb,
                reverbAmount: mixingEffects.reverbAmount,
                hasStereoProcessing: mixingEffects.hasStereoProcessing,
                stereoEnhancement: mixingEffects.stereoEnhancement,
                hasEQ: mixingEffects.hasEQ,
                eqBalance: mixingEffects.frequencyBalance
            )
            print("   üé∏ Stem metrics prepared for ChatGPT analysis (with mixing effects)")
        } else {
            // No stem analysis, but send mixing effects detection (critical for unmixed detection!)
            stemMetricsForAI = StemMetrics(
                vocalsLevel: 0,
                drumsLevel: 0,
                bassLevel: 0,
                otherLevel: 0,
                mixDepth: 0,
                foregroundClarity: 0,
                elementSeparation: 0,
                frequencyMasking: 0,
                mixDensity: 0,
                vocalsStereoWidth: 0,
                drumsStereoWidth: 0,
                bassStereoWidth: 0,
                vocalsPlacement: "unknown",
                drumsPlacement: "unknown",
                bassPlacement: "unknown",
                vocalsToInstrumentsRatio: 0,
                drumsToMixRatio: 0,
                bassToMixRatio: 0,
                hasCompression: mixingEffects.hasCompression,
                compressionAmount: mixingEffects.compressionAmount,
                hasReverb: mixingEffects.hasReverb,
                reverbAmount: mixingEffects.reverbAmount,
                hasStereoProcessing: mixingEffects.hasStereoProcessing,
                stereoEnhancement: mixingEffects.stereoEnhancement,
                hasEQ: mixingEffects.hasEQ,
                eqBalance: mixingEffects.frequencyBalance
            )
            print("   üéØ Basic metrics prepared for ChatGPT (mixing effects only - critical for unmixed detection)")
        }
        
        // Use ChatGPT Audio Analysis with native audio input
        print("   ü§ñ Analyzing with ChatGPT Audio API \(isProUser ? "(Pro)" : "(Free)")...")
        let chatGPTService = ChatGPTAudioAnalysisService()
        let chatGPTResponse = try await chatGPTService.analyzeAudio(
            fileURL: actualURL,
            isProUser: isProUser,
            stemMetrics: stemMetricsForAI
        )
        
        // Convert ChatGPT response to expected format
        let aiResponse = OpenAIAnalysisResponse(
            overallQuality: chatGPTResponse.overallScore,
            stereoAnalysis: chatGPTResponse.stereoWidth.analysis,
            frequencyAnalysis: chatGPTResponse.frequencyBalance.analysis,
            dynamicsAnalysis: chatGPTResponse.dynamicRange.analysis,
            effectsAnalysis: chatGPTResponse.loudness.analysis,
            recommendations: chatGPTResponse.recommendations,
            detailedSummary: chatGPTResponse.detailedSummary
        )
        
        print("   üì• ChatGPT Response:")
        print("      Score: \(aiResponse.overallQuality)")
        print("      Stereo: \(aiResponse.stereoAnalysis)")
        print("      Frequency: \(aiResponse.frequencyAnalysis)")
        print("      Dynamics: \(aiResponse.dynamicsAnalysis)")
        
        // Calculate a technical score based on objective metrics as a sanity check
        var technicalScore: Double = 100.0
        
        // Stereo width check (30-75% is ideal)
        let stereoWidthPercent = Double(stereoFeatures.stereoWidth * 100)
        if stereoWidthPercent < 25 {
            technicalScore -= 20  // Very narrow stereo
        } else if stereoWidthPercent < 30 {
            technicalScore -= 10  // Narrow stereo
        } else if stereoWidthPercent > 80 {
            technicalScore -= 15  // Too wide (phase issues likely)
        }
        
        // Phase coherence check (>0.5 is acceptable, >0.6 is good)
        if stereoFeatures.correlation < 0.4 {
            technicalScore -= 25  // Severe phase issues
        } else if stereoFeatures.correlation < 0.5 {
            technicalScore -= 15  // Phase issues
        }
        // 0.5-1.0 is fine, no penalty
        
        // Dynamic range check (4-18 dB is acceptable)
        if loudnessFeatures.dynamicRange < 3 {
            technicalScore -= 20  // Severely over-compressed
        } else if loudnessFeatures.dynamicRange < 4 {
            technicalScore -= 10  // Over-compressed
        } else if loudnessFeatures.dynamicRange > 20 {
            technicalScore -= 15  // Overly dynamic
        } else if loudnessFeatures.dynamicRange > 18 {
            technicalScore -= 5   // Very dynamic
        }
        
        // Peak level check (should be reasonably loud)
        if loudnessFeatures.peakLevel > 1.0 {
            technicalScore -= 25  // Clipping
        } else if peakLevelDB < -12 {
            technicalScore -= 15  // Very quiet
        } else if peakLevelDB < -10 {
            technicalScore -= 5   // Quiet
        }
        
        // Frequency balance check - CRITICAL for mix quality
        // Calculate percentages of total energy
        let totalFreqEnergy = lowEnergy + midEnergy + highEnergy
        if totalFreqEnergy > 0 {
            let lowPercent = Double(lowEnergy / totalFreqEnergy * 100)
            let midPercent = Double(midEnergy / totalFreqEnergy * 100)
            let highPercent = Double(highEnergy / totalFreqEnergy * 100)
            
            print("   üìä Checking frequency balance:")
            print("      Low: \(String(format: "%.1f", lowPercent))%")
            print("      Mid: \(String(format: "%.1f", midPercent))%")
            print("      High: \(String(format: "%.1f", highPercent))%")
            
            // Ideal range: 25-45% per band (with some flexibility)
            // Severely imbalanced: >60% or <15% in any band
            if lowPercent > 60 {
                technicalScore -= 30  // Extremely bass-heavy
                print("      ‚ùå Extremely bass-heavy: -30 points")
            } else if lowPercent > 50 {
                technicalScore -= 20  // Very bass-heavy
                print("      ‚ö†Ô∏è Very bass-heavy: -20 points")
            } else if lowPercent < 15 {
                technicalScore -= 20  // Lacking bass
                print("      ‚ö†Ô∏è Lacking bass: -20 points")
            }
            
            if midPercent < 20 {
                technicalScore -= 25  // Severely lacking mids (vocals, instruments)
                print("      ‚ùå Severely lacking mids: -25 points")
            } else if midPercent < 25 {
                technicalScore -= 15  // Lacking mids
                print("      ‚ö†Ô∏è Lacking mids: -15 points")
            } else if midPercent > 60 {
                technicalScore -= 20  // Too mid-heavy
                print("      ‚ö†Ô∏è Too mid-heavy: -20 points")
            }
            
            if highPercent > 50 {
                technicalScore -= 25  // Extremely harsh/bright
                print("      ‚ùå Extremely harsh/bright: -25 points")
            } else if highPercent > 45 {
                technicalScore -= 15  // Very bright
                print("      ‚ö†Ô∏è Very bright: -15 points")
            } else if highPercent < 10 {
                technicalScore -= 20  // Dull/muddy
                print("      ‚ö†Ô∏è Dull/muddy mix: -20 points")
            }
        }
        
        print("   üî¨ Technical Score (objective): \(Int(technicalScore))")
        print("   ü§ñ AI Score: \(Int(aiResponse.overallQuality))")

        // Determine if this is an unmixed track
        let isUnmixed = aiResponse.overallQuality < 51
        
        // SCORING STRATEGY:
        // - For professional mixes (AI score 70+), trust the AI's assessment
        // - For poor mixes (AI score < 60), use technical score as a sanity check
        // - For unmixed tracks (AI score < 51), ensure score stays in unmixed range
        
        let finalScore: Double
        
        if isUnmixed {
            // Unmixed track: cap at 50, use minimum of both scores
            finalScore = min(min(technicalScore, 50.0), aiResponse.overallQuality)
            print("   ‚≠ê Final Score (UNMIXED - capped): \(Int(finalScore))")
        } else if aiResponse.overallQuality >= 70 {
            // Professional/good mix: Trust AI's assessment, only cap if technical score is significantly lower
            // Allow AI score unless technical issues are severe (technical < 60)
            if technicalScore < 60 && technicalScore < aiResponse.overallQuality - 15 {
                finalScore = (aiResponse.overallQuality + technicalScore) / 2  // Average if big discrepancy
                print("   ‚≠ê Final Score (averaged due to technical issues): \(Int(finalScore))")
            } else {
                finalScore = aiResponse.overallQuality  // Trust AI for professional mixes
                print("   ‚≠ê Final Score (AI - professional mix): \(Int(finalScore))")
            }
        } else {
            // Fair/poor mix (60-69): Use average to be conservative
            finalScore = (aiResponse.overallQuality + technicalScore) / 2
            print("   ‚≠ê Final Score (averaged): \(Int(finalScore))")
        }
        
        // Create analysis result
        analysisProgress = 0.9
        let result = AnalysisResult(audioFile: audioFile, analysisVersion: "OpenAI-1.0")
        
        // Populate technical metrics
        result.stereoWidthScore = Double(stereoFeatures.stereoWidth * 100)
        result.phaseCoherence = Double(stereoFeatures.correlation)
        result.dynamicRange = Double(loudnessFeatures.dynamicRange)
        result.loudnessLUFS = Double(loudnessFeatures.lufs)
        result.peakLevel = Double(peakLevelDB)
        result.rmsLevel = Double(rmsLevelDB)
        result.truePeakLevel = Double(peakLevelDB)  // For now, use same as peak; true peak requires oversampling
        result.spectralCentroid = Double(frequencyFeatures.spectralCentroid)
        result.hasClipping = loudnessFeatures.peakLevel >= 1.0
        
        // Normalize frequency bands to percentages (0-100)
        // Calculate total energy and convert each band to percentage of total
        let totalEnergy = lowEnergy + midEnergy + highEnergy
        if totalEnergy > 0 {
            result.lowEndBalance = Double((lowEnergy / totalEnergy) * 100)
            result.midBalance = Double((midEnergy / totalEnergy) * 100)
            result.highBalance = Double((highEnergy / totalEnergy) * 100)
        } else {
            // Fallback to equal distribution if no energy detected
            result.lowEndBalance = 33.3
            result.midBalance = 33.3
            result.highBalance = 33.3
        }
        
        print("   üìä Frequency Balance (normalized):")
        print("      Low: \(result.lowEndBalance)%")
        print("      Mid: \(result.midBalance)%")
        print("      High: \(result.highBalance)%")
        
        // Calculate frequency balance score based on actual distribution
        // Perfect balance = 33.3% each band
        // Score based on deviation from ideal
        let lowDeviation = abs(result.lowEndBalance - 33.3)
        let midDeviation = abs(result.midBalance - 33.3)
        let highDeviation = abs(result.highBalance - 33.3)
        let totalDeviation = lowDeviation + midDeviation + highDeviation
        
        // Convert deviation to score (0 deviation = 100 score, 50+ deviation = 0 score)
        let calculatedBalanceScore = max(0, min(100, 100 - (totalDeviation * 2)))
        
        // Determine balance status
        let balanceStatus: String
        if calculatedBalanceScore >= 85 {
            balanceStatus = "balanced"
        } else if result.lowEndBalance > 45 {
            balanceStatus = "bass-heavy"
        } else if result.midBalance > 45 {
            balanceStatus = "mid-heavy"
        } else if result.highBalance > 45 {
            balanceStatus = "treble-heavy"
        } else {
            balanceStatus = "moderate"
        }
        
        // Use calculated score instead of feature extractor score
        result.frequencyBalanceScore = calculatedBalanceScore
        result.frequencyBalanceStatus = balanceStatus
        
        // Populate frequency balance analysis from extracted features
        let freqBalance = frequencyFeatures.frequencyBalance
        result.lowFrequencyPercent = Double(freqBalance.lowEnergy * 100)
        result.midFrequencyPercent = Double(freqBalance.midEnergy * 100)
        result.highFrequencyPercent = Double(freqBalance.highEnergy * 100)
        
        print("   üéöÔ∏è Frequency Balance Analysis:")
        print("      Score: \(Int(result.frequencyBalanceScore))% (calculated from distribution)")
        print("      Status: \(result.frequencyBalanceStatus)")
        print("      Deviation: Low=\(String(format: "%.1f", lowDeviation))%, Mid=\(String(format: "%.1f", midDeviation))%, High=\(String(format: "%.1f", highDeviation))%")
        
        // Populate stem-based analysis if available
        if let stemAnalysis = stemAnalysis, let mixBalance = mixBalance {
            result.hasStemAnalysis = true
            result.vocalsLevel = Double(mixBalance.vocalsLevel)
            result.drumsLevel = Double(mixBalance.drumsLevel)
            result.bassLevel = Double(mixBalance.bassLevel)
            result.otherInstrumentsLevel = Double(mixBalance.otherLevel)
            
            result.mixDepthScore = Double(stemAnalysis.depthScore * 100)
            result.foregroundClarityScore = Double(stemAnalysis.foregroundClarity * 100)
            result.elementSeparationScore = Double(stemAnalysis.elementSeparationQuality * 100)
            result.backgroundAmbienceScore = Double(mixingEffects.reverbAmount * 100)
            
            result.vocalsStereoWidth = Double(mixBalance.vocalsStereoWidth * 100)
            result.drumsStereoWidth = Double(mixBalance.drumsStereoWidth * 100)
            result.bassStereoWidth = Double(mixBalance.bassStereoWidth * 100)
            
            result.vocalsPlacement = stemAnalysis.vocalsPlacement
            result.drumsPlacement = stemAnalysis.drumsPlacement
            result.bassPlacement = stemAnalysis.bassPlacement
            
            result.frequencyMaskingScore = Double(stemAnalysis.frequencyMasking * 100)
            result.mixDensityScore = Double(stemAnalysis.mixDensity * 100)
            
            print("   üé∏ Stem-Based Analysis:")
            print("      Vocals Level: \(Int(result.vocalsLevel * 100))%")
            print("      Drums Level: \(Int(result.drumsLevel * 100))%")
            print("      Bass Level: \(Int(result.bassLevel * 100))%")
            print("      Mix Depth: \(Int(result.mixDepthScore))%")
            print("      Foreground Clarity: \(Int(result.foregroundClarityScore))%")
            print("      Element Separation: \(Int(result.elementSeparationScore))%")
        } else {
            result.hasStemAnalysis = false
            print("   ‚ÑπÔ∏è Stem analysis not performed")
        }
        
        // Apply OpenAI analysis (use our calculated final score instead of raw AI score)
        result.overallScore = finalScore
        print("   üéØ DEBUG: finalScore = \(finalScore), result.overallScore = \(result.overallScore)")
        
        // If mix is excellent (85+), no need for recommendations
        if finalScore >= 85 {
            result.recommendations = ["Your mix sounds excellent! No significant improvements needed."]
        } else if finalScore >= 75 {
            // Good mix - only keep critical recommendations (max 3)
            result.recommendations = Array(aiResponse.recommendations.prefix(3))
        } else {
            // Fair/poor mix - show all recommendations
            result.recommendations = aiResponse.recommendations
        }
        
        // Set issue flags based on REALISTIC professional thresholds
        // Phase coherence: <0.3 is severe, 0.3-0.7 is acceptable, 0.7+ is good
        result.hasPhaseIssues = stereoFeatures.correlation < 0.3
        
        // Stereo width: <30% is narrow, 30-75% is good, >80% is too wide
        result.hasStereoIssues = stereoFeatures.stereoWidth < 0.3 || stereoFeatures.stereoWidth > 0.8
        
        // Frequency imbalance: Check both the balance score and individual bands
        // Flag if score is poor (<50%) OR any band is severely dominant (>60%)
        result.hasFrequencyImbalance = (result.frequencyBalanceScore < 50 || lowEnergy > 0.6 || highEnergy > 0.6)
        
        // Dynamic range: 4-18 dB is acceptable range
        result.hasDynamicRangeIssues = (loudnessFeatures.dynamicRange < 4.0 || 
                                       loudnessFeatures.dynamicRange > 18.0)
        
        // Store AI-generated analysis text
        result.stereoAnalysis = aiResponse.stereoAnalysis
        result.frequencyAnalysis = aiResponse.frequencyAnalysis
        result.dynamicsAnalysis = aiResponse.dynamicsAnalysis
        result.detailedSummary = aiResponse.detailedSummary
        
        print("   ‚úÖ OpenAI Analysis Complete:")
        print("      Overall Quality: \(aiResponse.overallQuality)/100")
        print("      Stereo: \(aiResponse.stereoAnalysis)")
        print("      Frequency: \(aiResponse.frequencyAnalysis)")
        print("      Dynamics: \(aiResponse.dynamicsAnalysis)")
        print("      Summary: \(aiResponse.detailedSummary)")
        print("      Recommendations: \(aiResponse.recommendations.count)")
        
        analysisProgress = 1.0
        
        return result
    }
}
